<html>
<body>
  <center>
<h2> Chong Wang </h2>
</center>
  
<table border="0" cellpadding="5" cellspacing="10">
<tbody><tr>
<td valign="middle">
<img src="chongw.jpg" height="150">
</td>
<td valign="middle">
    
<!-- Before Google, I worked at <a
  href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> and  <a
href="http://research.baidu.com/">Baidu Silicon Valley AI Lab</a>.  
 -->
  
<p>I am currently working at <a href="https://www.bytedance.com">ByteDance</a> as head of the applied machine learning (AML) research team. Previously, I worked as a research scientist at 
  <a href="https://www.google.com">Google</a>. Even earlier, I received my PhD from <a href="https://www.cs.princeton.edu/">Computer Science Department, Princeton University</a>, working with Prof. <a href="http://www.cs.columbia.edu/~blei/">David Blei</a>. </p>
  
  <p> Our team works on fundamental machine learning research and its applications for many of our products, such as <a href="https://www.tiktok.com">TikTok</a> and Douyin, among others. We are actively hiring research engineers, scientists and interns. 
    Please apply here: 
    <a href="https://jobs.bytedance.com/referral/pc/position/detail/?token=MTsxNjI4ODc0NjY4MDg5OzY2ODgyMTQyNzc0NDAwMzAyMTk7Njk3MDA2NDA1MjkyODM4MzIzOQ">research scientist</a> and 
    <a href="https://jobs.bytedance.com/referral/pc/position/detail/?token=MTsxNjI4ODc0Njc5Mzk1OzY2ODgyMTQyNzc0NDAwMzAyMTk7Njk3MDA2NTIyMDU1NTUyNDM4Mw">research engineer</a>. 
    If you are seeking an internship, you can apply here:
    <a href="https://careers.tiktok.com/position/7016763506712611079/detail">internship</a>. Feel free to use my referral code: UN7TKMA.  
  
<p>email: chong.wang@[companyname].com</p>
</td>
  </tr>
</tbody></table>

 <h4>Some Awards:</h4>
 <ul>
<!-- <li>ACM Doctoral Dissertation Award Nomination (by Princeton University), 2012</li> -->
 <li>SIGKDD 2021 <a href="https://kdd.org/awards/view/2021-sigkdd-test-of-time-award-winners"> Test of Time Award</a> </li>
 <li>Google PhD fellow and Siebel Scholar</li>
 <li>Paper awards or honorable mentions at NIPS, AISTATS, KDD</li>
 </ul> 
<h4>Highlights:</h4>
<ul> 
    <li>
   A more accurate approximation to softmax attention with linear complexity,
    <ul>
      <li> <a href="https://arxiv.org/abs/2204.04667">Linear Complexity Randomized Self-attention Mechanism</a>, 2022</li>
    </ul>
  </li>
  
  <li>
   A more efficient nonuniform negative sampling algorithm,
    <ul>
      <li> <a href="https://arxiv.org/abs/2110.13048">Nonuniform Negative Sampling and Log Odds Correction with Rare Events Data</a>, NeurIPS 2021</li>
    </ul>
  </li>
  <li>
   Learning large-scale end-to-end retrieval models without resorting to approximate nearest neighbor search,
    <ul>
      <li><a href="https://arxiv.org/abs/2007.07203">Deep Retrieval: An End-to-End Learnable Structure Model for Large-Scale Recommendations</a>, CIKM 2021.</li>
    </ul>
  </li>
  <li> Neural networks meet structured Bayesian methods, 
     <ul>
       <li> <a href="https://arxiv.org/abs/1810.04719"> Fully Supervised Speaker Diarization</a> provides better accuracy and efficiency in speaker diarization, ICASSP 2019.
         <ul>
         <li><a href="https://ai.googleblog.com/2018/11/accurate-online-speaker-diarization.html">Official Google AI blog</a>.</li>
         <li> Media reports: <a href="https://venturebeat.com/2018/11/12/google-open-sources-ai-that-can-distinguish-between-voices-with-92-percent-accuracy/"> VentureBeat</a>, 
         <a href="https://siliconangle.com/2018/11/12/google-built-ai-model-can-accurately-distinguish-different-human-voices/">SiliconANGLE</a>, 
           <a href="https://www.infoq.com/news/2018/11/Google-AI-Voice">InfoQ</a>,
           <a href="https://futurism.com/the-byte/google-ai-recognize-new-voices">futurism</a>,
           <a href="https://www.cnbeta.com/articles/tech/787295.htm" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">cnBeta</a>,
<a href="https://tech.sina.com.cn/it/2018-11-13/doc-ihmutuea9658316.shtml" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">Sina Tech</a>,
<a href="https://www.ithome.com.tw/news/126984" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">iThome</a>,
<a href="http://www.chinaemail.com.cn/blog/content/9652/%E8%B0%B7%E6%AD%8C%E5%BC%80%E6%BA%90AI%E8%83%BD%E5%8C%BA%E5%88%86%E5%A3%B0%E9%9F%B3-%E5%87%86%E7%A1%AE%E7%8E%87%E8%BE%BE92%25.html" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">ChinaEmail</a>,
<a href="http://www.eepw.com.cn/article/201811/394377.htm" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">eepw</a>,
<a href="https://mp.weixin.qq.com/s/YOupCjU06JhRCZNCbMvAgQ" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">QbitAI</a>,
<a href="https://www.oschina.net/news/101780/fully-super-vised-speaker-diarization" rel="noreferrer noopener" class="g3doc-nong3doc-link g3doc-external-link">oschina</a>.</li>
           </li>
           <li><a href="https://github.com/google/uis-rnn">Open source on github with 1400+ stars!</a></li>
         </ul>
        </li>
    </ul>
  </li>
<li>A generic approach for sequence modeling through segmentations,
   <ul>
   <li> <a href="https://arxiv.org/abs/1702.07463">Sequence Modeling via Segmentations</a>, ICML 2017; </li> 
   <li> <a href="https://arxiv.org/abs/1706.05565">Towards Neural Phrase-based Machine Translation</a> (<a href="https://github.com/posenhuang/NPMT">code</a>), ICLR 2018; </li>
   <li> <a href="https://arxiv.org/abs/1804.07855">Subgoal Discovery for Hierarchical Dialogue Policy Learning</a>, EMNLP 2018. </li>
   <li> <a href="https://arxiv.org/abs/1811.02172">Neural Phrase-to-Phrase Machine Translation</a> on Arxiv. </li>
  </ul>
</li>
<li><a
    href="https://openreview.net/pdf?id=91EowxONgIkRlNvXUVog">Lookahead convolution
    architecture</a> (ICLR 2016 workshop) enabled the deployment of an<a
    href="http://jmlr.org/proceedings/papers/v48/amodei16.pdf"> end-to-end
    speech recognition system</a> (ICML 2016) to benefit hundreds of millions of users (<a href="http://maps.baidu.com">http://maps.baidu.com</a>) by
  significantly reducing the latency. See a <a
    href="https://github.com/torch/nn/blob/master/doc/convolution.md#nn.TemporalRowConvolution">torch
    implementation</a>. </li>
    <li><a href="http://www.cs.columbia.edu/~blei/papers/WangBlei2011.pdf">Collaborative topic models</a> (KDD 2011) are used by <a href="http://www.nytimes.com">New York Times</a> for <a href="http://open.blogs.nytimes.com//2015/08/11/building-the-next-new-york-times-recommendation-engine/">their recommendation engine</a>.</li>
</ul>
<h4>Publications:</h4>
<ul>
  <li> Find me at <a href="https://scholar.google.com/citations?user=vRI2blsAAAAJ&hl=en">Google scholar</a> and <a href="https://www.linkedin.com/in/chong-wang-7512902/">LinkedIn</a>.
</ul>
  
  <h4>Recent News:</h4>
<ul>
  <li>04/2022: <a href="https://arxiv.org/abs/2204.04667">Linear Complexity Randomized Self-attention Mechanism</a> on arxiv, 2022.</li>
  <li>03/2022: <a href="https://arxiv.org/abs/2203.02073">Differentially Private Label Protection in Split Learning</a> on arxiv, 2022.</li>
  <li>01/2022: <a href="https://arxiv.org/abs/2102.08504">Label Leakage and Protection in Two-party Split Learning</a> accepted to ICLR, 2022.</li>
  <li>11/2021: <a href="https://arxiv.org/abs/2111.15176">Learning Large-Time-Step Molecular Dynamics with Graph Neural Networks</a> presented at AI for Science workshop in NeurIPS 2021.</li>
  <li>09/2021: <a href="https://arxiv.org/abs/2110.13048">Nonuniform Negative Sampling and Log Odds Correction with Rare Events Data</a> accepted to NeurIPS 2021. </li>
  <li>08/2021: <a href="https://arxiv.org/abs/2007.07203">Deep Retrieval: An End-to-End Learnable Structure Model for Large-Scale Recommendations</a> accepted to CIKM 2021. </li>
  <li>05/2019: <a href="http://arxiv.org/abs/1810.06401">Rate Distortion For Model Compression: From Theory To Practice</a> accepted to ICML 2019. </li>
  <li>02/2019: <a href="https://arxiv.org/abs/1810.04719">Fully Supervised Speaker Diarization</a> accepted to ICASSP 2019. </li>
  <li>12/2018: <a href="https://openreview.net/forum?id=B1xY-hRctX">Neural Logic Machines</a> accepted to ICLR 2019. </li>
  <li>11/2018: Invited <a href="https://www.cs.washington.edu/talks/chong-wang">talk</a> at University of Washington, Seattle.</li>
  <li>10/2018: <a href="https://arxiv.org/abs/1810.04719"> Fully Supervised Speaker Diarization</a> submitted to Arxiv, perhaps the first supervised method doing so! </li>
  <li>08/2018: <a href="https://arxiv.org/abs/1804.07855"> Subgoal Discovery for Hierarchical Dialogue Policy Learning</a> to appear at EMNLP (oral presentation), 2018.</li>
  <li>06/2018: Invited talk at machine learning thoery workshop, Peking University, Beijing, China.</li>
  <li>03/2018: Invited <a href="http://cms.caltech.edu/events/81414">talk</a> at Caltech, Pasadena, CA.</li>
  <li>02/2018: <a href="https://arxiv.org/abs/1711.06373">Thoracic Disease Identification and Localization with Limited Supervision</a> to appear at CVPR, 2018.</li>
  <li>01/2018: <a href="https://arxiv.org/abs/1706.05565">Towards Neural Phrase-based Machine Translation</a> to appear at ICLR, 2018. (<a href="https://github.com/posenhuang/NPMT">code</a>)</li>
  <li>01/2018: <a href="https://projecteuclid.org/euclid.ba/1514970064">A General Method for Robust Bayesian Modeling</a> to appear at Journal of Bayesian Analysis.</li>
  <li>09/2017: <a href="https://papers.nips.cc/paper/7083-q-lda-uncovering-latent-patterns-in-text-based-sequential-decision-processes">Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes</a> to appear at NIPS, 2017.</li>
</ul>

<h4>Fun stuff:</h4> 
<ul>
<li> <a href="https://www.wattpad.com/user/QueenSwirly">Stories written by my daughter Amy</a> on Wattpad. </li>
</ul>
  
</body>
</html>
